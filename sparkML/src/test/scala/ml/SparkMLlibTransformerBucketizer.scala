package ml

import org.apache.spark.ml.feature.Bucketizer
import util.SparkTools

/**
  * 分箱（分段处理）：将连续数值转换为离散类别
  * 比如特征是年龄，是一个连续数值，需要将其转换为离散类别(未成年人、青年人、中年人、老年人），就要用到Bucketizer了。
  * 分类的标准是自己定义的，在Spark中为split参数,定义如下：
  * double[] splits = {0, 18, 35,50， Double.PositiveInfinity}
  * 将数值年龄分为四类0-18，18-35，35-50，55+四个段。
  * 如果左右边界拿不准，就设置为，Double.NegativeInfinity， Double.PositiveInfinity，不会有错的
  */
object SparkMLlibTransformerBucketizer extends SparkTools{
  // [-inf, 0.5, 2.5, 3.5, 4.5, 5.5, inf]
  val splits = Array[Double](Double.NegativeInfinity,2.0, 3.0, 5.0, 6.0, 8.0, 10.0, 12.0, 16.0, 21.0, Double.PositiveInfinity)
//  val splits = Array[Double](Double.NegativeInfinity,0.5, 2.5, 3.5, 4.5, 5.5, Double.PositiveInfinity)
  val bucketizer = new Bucketizer().setInputCol("m1").setOutputCol("m1_bin").setSplits(splits)
  val res = bucketizer.transform(baseDf)

  res.show()

  /**source
    *       d14    ad  day7    m1    m3    m6   m12   m18    m24    m60  \
    * 0     0  2018-06-24  -1.0   2.0   6.0  13.0  42.0  48.0   54.0   54.0
    * 1     0  2018-06-24   4.0   5.0  12.0  21.0  67.0  73.0   80.0   80.0
    * 2     0  2018-06-24   3.0  10.0  25.0  36.0  66.0  68.0   68.0   68.0
    * 3     0  2018-06-24  -1.0  16.0  33.0  33.0  33.0  33.0   35.0   35.0
    * 4     0  2018-06-24  -1.0   2.0   7.0  30.0  33.0  36.0   36.0   36.0
    * 5     0  2018-06-24   3.0   7.0  10.0  24.0  36.0  41.0   41.0   41.0
    * 6     0  2018-06-24   3.0   5.0  25.0  28.0  35.0  35.0   35.0   35.0
    * 7     0  2018-06-24   5.0  20.0  32.0  39.0  40.0  40.0   40.0   40.0
    * 8     0  2018-06-24   5.0  15.0  19.0  30.0  54.0  89.0   93.0   97.0
    * 9     0  2018-06-24  -1.0   6.0  19.0  37.0  52.0  64.0   65.0   65.0
    * 10    0  2018-06-24  -1.0  -1.0   2.0   9.0   9.0   9.0   10.0   10.0
    * 11    0  2018-06-24  -1.0   3.0  27.0  27.0  27.0  27.0   27.0   27.0
    * 12    0  2018-06-24   9.0  28.0  62.0  81.0  86.0  87.0   87.0   87.0
    * 13    0  2018-06-24   2.0   5.0  11.0  26.0  29.0  29.0   29.0   29.0
    * 14    0  2018-06-24   2.0  18.0  35.0  56.0  80.0  98.0  106.0  106.0
    * 15    0  2018-06-24  14.0  27.0  63.0  65.0  82.0  84.0   84.0   84.0
    * 16    0  2018-06-24  -1.0   3.0  14.0  21.0  26.0  26.0   26.0   26.0
    * 17    0  2018-06-24   5.0  18.0  20.0  22.0  47.0  56.0   56.0   56.0
    * 18    0  2018-06-24   2.0   7.0   8.0   9.0  31.0  32.0   32.0   32.0
    * 19    0  2018-06-24   7.0  13.0  38.0  53.0  60.0  60.0   60.0   61.0
    */

  /** python
    * day7 [-inf, 2.0, 3.0, 4.0, 5.0, 8.0, inf]
    *          0        1            2          3              4          5              6
    *    [-inf,2.0) [2.0, 3.0) , [3.0, 5.0)  [5.0,6.0) , [6.0, 8.0), [8.0, 10.0), [10.0, 12.0)  spark 使用的是左闭右开,算法二分查找  索引位置后插入，需有修改为前插入的索引号
    *    (-inf,2.0] (2.0, 3.0] , (3.0, 5.0]  (5.0,6.0],  (6.0, 8.0], (8.0, 10.0], (10,0, 12.0]  python
    * m1 [-inf, 2.0, 3.0, 5.0, 6.0, 8.0, 10.0, 12.0, 16.0, 21.0, inf]
    *
    * m3 [-inf, 7.0, 11.0, 15.0, 18.0, 22.0, 26.0, 31.0, 37.0, 45.0, inf]
    * m6 [-inf, 12.0, 17.0, 22.0, 27.0, 31.0, 36.0, 42.0, 49.0, 58.0, inf]
    * m12 [-inf, 20.0, 28.0, 34.0, 40.0, 46.0, 53.0, 60.0, 69.0, 82.0, inf]
    * m18 [-inf, 22.0, 30.0, 37.0, 44.0, 50.0, 58.0, 66.0, 76.0, 92.0, inf]
    * m24 [-inf, 23.0, 31.0, 38.0, 45.0, 52.0, 59.0, 68.0, 78.0, 94.0, inf]
    * m60 [-inf, 23.0, 32.0, 39.0, 45.0, 52.0, 60.0, 68.0, 79.0, 95.0, inf]
    *
    *         day7_bin  m1_bin  m3_bin  m6_bin  m12_bin  m18_bin  m24_bin  m60_bin
    * 0          0       0       0       1        4        4        5        5
    * 1          2       2       2       2        7        7        8        8
    * 2          1       5       5       5        7        7        6        6
    * 3          0       7       7       5        2        2        2        2
    * 4          0       0       0       4        2        2        2        2
    * 5          1       4       1       3        3        3        3        3
    * 6          1       2       5       4        3        2        2        2
    * 7          3       8       7       6        3        3        3        3
    * 8          3       7       4       4        6        8        8        9
    * 9          0       3       4       6        5        6        6        6
    * 10         0       0       0       0        0        0        0        0
    * 11         0       1       6       3        1        1        1        1
    * 12         5       9       9       9        9        8        8        8
    * 13         0       2       1       3        2        1        1        1
    * 14         0       8       7       8        8        9        9        9
    * 15         5       9       9       9        8        8        8        8
    * 16         0       1       2       2        1        1        1        1
    * 17         3       8       4       2        5        5        5        5
    * 18         0       4       1       0        2        2        2        1
    * 19         4       7       8       8        6        6        6        6
    */


//  def transform(df: DataFrame): DataFrame = {
//    var tmp = df
//    for (c <- binCols) {
//      val bucketizer: UserDefinedFunction = udf { feature: Double =>
//        binarySearchForBuckets(binsMap(c), feature, keepInvalid = true)
//      }
//      tmp = tmp.withColumn(c + "_bin", bucketizer(df(c)))
//    }
//    tmp
//  }
}
